[{"title":"page","url":"http://yoursite.com/2018/08/14/page/","content":"","categories":[],"tags":[]},{"title":"内存管理与垃圾回收","url":"http://yoursite.com/2018/08/12/内存管理与垃圾回收/","content":"<p>最近在看《深入理解Java虚拟机 JVM高级特性与最佳实践》，对其中的重要知识点进行一个总结。</p>\n<h2 id=\"运行时的数据区\"><a href=\"#运行时的数据区\" class=\"headerlink\" title=\"运行时的数据区\"></a>运行时的数据区</h2><p><img src=\"http://ojr4jzv5e.bkt.clouddn.com/vernJVM%E8%BF%90%E8%A1%8C%E6%97%B6%E6%95%B0%E6%8D%AE%E5%8C%BA.jpg\"></p>\n<h2 id=\"垃圾收集器\"><a href=\"#垃圾收集器\" class=\"headerlink\" title=\"垃圾收集器\"></a>垃圾收集器</h2><h3 id=\"如何判断对象已死\"><a href=\"#如何判断对象已死\" class=\"headerlink\" title=\"如何判断对象已死\"></a>如何判断对象已死</h3><h4 id=\"引用计数算法\"><a href=\"#引用计数算法\" class=\"headerlink\" title=\"引用计数算法\"></a>引用计数算法</h4><p>引用计数算法是垃圾收集器中的早期策略。在这种方法中，堆中的每个对象实例都有一个引用计数。当一个对象被创建时，将该对象实例分配给一个引用变量，该对象实例的引用计数设置为 1。当任何其它变量被赋值为这个对象的引用时，对象实例的引用计数加1（a=b，则b引用的对象实例的计数器加1），但当一个对象实例的某个引用超过了生命周期或者被设置为一个新值时，对象实例的引用计数减1。特别地，当一个对象实例被垃圾收集时，它引用的任何对象实例的引用计数器均减1。任何引用计数为0的对象实例可以被当作垃圾收集。</p>\n<p>引用计数收集器可以很快的执行，并且交织在程序运行中，对程序需要不被长时间打断的实时环境比较有利，但其很难解决对象之间相互循环引用的问题。</p>\n<h4 id=\"可达性分析算法\"><a href=\"#可达性分析算法\" class=\"headerlink\" title=\"可达性分析算法\"></a>可达性分析算法</h4><p>可达性分析算法是通过判断对象的引用链是否可达来决定对象是否可以被回收。可达性分析算法是从离散数学中的图论引入的，程序把所有的引用关系看作一张图，通过一系列的名为 “GC Roots” 的对象作为起始点，从这些节点开始向下搜索，搜索所走过的路径称为引用链（Reference Chain）。当一个对象到 GC Roots 没有任何引用链相连（用图论的话来说就是从 GC Roots 到这个对象不可达）时，则证明此对象是不可用的，如下图所示。在Java中，可作为 GC Root 的对象包括以下几种：</p>\n<ol>\n<li>虚拟机栈(栈帧中的局部变量表)中引用的对象；</li>\n<li>方法区中类静态属性引用的对象；</li>\n<li>方法区中常量引用的对象； </li>\n<li>本地方法栈中Native方法引用的对象.</li>\n</ol>\n<h4 id=\"再谈引用\"><a href=\"#再谈引用\" class=\"headerlink\" title=\"再谈引用\"></a>再谈引用</h4><p>Java对引用的概念进行了扩充，将引用分为强引用（Strong Reference）、软引用（Soft Reference）、弱引用（Weak Reference）、虚引用（Phantom Reference）4种，这4种引用强度依次逐渐减弱。</p>\n<ul>\n<li><p><strong>强引用</strong>就是指在程序代码之中普遍存在的，类似“Object obj = new Object()”这类的引用，只要强引用还存在，垃圾收集器永远不会回收掉被引用的对象。</p>\n</li>\n<li><p><strong>软引用</strong>是用来描述一些还有用但并非必需的对象。对于软引用关联着的对象，在系统将要发生内存溢出异常之前，将会把这些对象列进回收范围之中进行第二次回收。如果这次回收还没有足够的内存，才会抛出内存溢出异常。在JDK 1.2之后，提供了SoftReference类来实现软引用。</p>\n</li>\n<li><p><strong>弱引用</strong>也是用来描述非必需对象的，但是它的强度比软引用更弱一些，被弱引用关联的对象只能生存到下一次垃圾收集发生之前。当垃圾收集器工作时，无论当前内存是否足够，都会回收掉只被弱引用关联的对象。在JDK 1.2之后，提供了WeakReference类来实现弱引用。</p>\n</li>\n<li><p><strong>虚引用</strong>也称为幽灵引用或者幻影引用，它是最弱的一种引用关系。一个对象是否有虚引用的存在，完全不会对其生存时间构成影响，也无法通过虚引用来取得一个对象实例。为一个对象设置虚引用关联的唯一目的就是能在这个对象被收集器回收时收到一个系统通知。在JDK 1.2之后，提供了PhantomReference类来实现虚引用。</p>\n</li>\n</ul>\n<h3 id=\"垃圾收集算法\"><a href=\"#垃圾收集算法\" class=\"headerlink\" title=\"垃圾收集算法\"></a>垃圾收集算法</h3><h4 id=\"标记-清除算法\"><a href=\"#标记-清除算法\" class=\"headerlink\" title=\"标记-清除算法\"></a>标记-清除算法</h4><p>算法分为标记和清除两个阶段：首先标记出所有需要回收的对象，在标记完成后统一回收所有被标记的对象，它的标记过程就是使用可达性算法进行标记的。</p>\n<p>主要缺点有两个：</p>\n<ul>\n<li>效率问题，标记和清除两个过程的效率都不高</li>\n<li>空间问题，标记清除之后会产生大量不连续的内存碎片</li>\n</ul>\n<p><img src=\"http://ojr4jzv5e.bkt.clouddn.com/vern%E6%A0%87%E8%AE%B0%E6%B8%85%E9%99%A4%E7%AE%97%E6%B3%95.jpg\"></p>\n<h4 id=\"复制算法\"><a href=\"#复制算法\" class=\"headerlink\" title=\"复制算法\"></a>复制算法</h4><p>复制算法将可用内存按容量划分为大小相等的两块，每次只使用其中的一块。当这一块的内存用完了，就将还存活着的对象复制到另外一块上面，然后再把已使用过的内存空间一次清理掉。这种算法适用于对象存活率低的场景，比如新生代。这样使得每次都是对整个半区进行内存回收，内存分配时也就不用考虑内存碎片等复杂情况，只要移动堆顶指针，按顺序分配内存即可，实现简单，运行高效。</p>\n<p><img src=\"http://ojr4jzv5e.bkt.clouddn.com/vern%E5%A4%8D%E5%88%B6%E7%AE%97%E6%B3%95.jpg\"></p>\n<h4 id=\"标记-整理算法\"><a href=\"#标记-整理算法\" class=\"headerlink\" title=\"标记-整理算法\"></a>标记-整理算法</h4><p>复制收集算法在对象存活率较高时就要进行较多的复制操作，效率将会变低。更关键的是，如果不想浪费50%的空间，就需要有额外的空间进行分配担保，以应对被使用的内存中所有对象都100%存活的极端情况，所以在老年代一般不能直接选用这种算法。标记整理算法的标记过程类似标记清除算法，但后续步骤不是直接对可回收对象进行清理，而是让所有存活的对象都向一端移动，然后直接清理掉端边界以外的内存，类似于磁盘整理的过程，该垃圾回收算法适用于对象存活率高的场景（老年代），其作用原理如下图所示。</p>\n<p><img src=\"http://ojr4jzv5e.bkt.clouddn.com/vern%E6%A0%87%E8%AE%B0%E6%95%B4%E7%90%86%E7%AE%97%E6%B3%95.jpg\"></p>\n<p>标记整理算法与标记清除算法最显著的区别是：标记清除算法不进行对象的移动，并且仅对不存活的对象进行处理；而标记整理算法会将所有的存活对象移动到一端，并对不存活对象进行处理，因此其不会产生内存碎片。</p>\n<h4 id=\"分代收集算法\"><a href=\"#分代收集算法\" class=\"headerlink\" title=\"分代收集算法\"></a>分代收集算法</h4><p>对于一个大型的系统，当创建的对象和方法变量比较多时，堆内存中的对象也会比较多，如果逐一分析对象是否该回收，那么势必造成效率低下。分代收集算法是基于这样一个事实：不同的对象的生命周期(存活情况)是不一样的，而不同生命周期的对象位于堆中不同的区域，因此对堆内存不同区域采用不同的策略进行回收可以提高 JVM 的执行效率。当代商用虚拟机使用的都是分代收集算法：新生代对象存活率低，就采用复制算法；老年代存活率高，就用标记清除算法或者标记整理算法。</p>\n<h3 id=\"G1收集器\"><a href=\"#G1收集器\" class=\"headerlink\" title=\"G1收集器\"></a>G1收集器</h3><h2 id=\"内存分配与回收策略\"><a href=\"#内存分配与回收策略\" class=\"headerlink\" title=\"内存分配与回收策略\"></a>内存分配与回收策略</h2><ol>\n<li>对象优先在Eden分配</li>\n<li>大对象直接进入老年代</li>\n<li>长期存活的对象将进入老年代</li>\n<li>动态对象年龄判定</li>\n<li>空间分配担保</li>\n</ol>\n<p><strong>参考</strong></p>\n<ol>\n<li><a href=\"https://vernlium.github.io/2015/10/11/%E3%80%8A%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Java%E8%99%9A%E6%8B%9F%E6%9C%BA%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/\" target=\"_blank\" rel=\"noopener\">《深入理解Java虚拟机》读书笔记</a></li>\n</ol>\n","categories":["技术"],"tags":["JVM"]},{"title":"分布式系统中的重要的协议","url":"http://yoursite.com/2018/08/06/分布式系统中的重要的协议/","content":"<p>分布式系统中有两个重要协议，包括Paxos选举协议以及两阶段提交协议。Paxos协议用于多个节点之间达成一致，往往用于总控节点的选举。两阶段提交协议用于保证跨多个节点操作的原子性，这些操作要么全部成功，要么全部失败。</p>\n<h2 id=\"1-两阶段提交协议\"><a href=\"#1-两阶段提交协议\" class=\"headerlink\" title=\"1.两阶段提交协议\"></a>1.两阶段提交协议</h2><p>两阶段提交协议（Two-phase Commit,2PC）通常用来实现分布式事务。在该协议中，系统包含两类节点：一类为协调者，通常只有一个，另一类为事务参与者，一般包含多个。</p>\n<p>在正常的执行过程中，两阶段提交协议包含以下两个过程：</p>\n<p>阶段1：请求阶段。在请求阶段，协调者通知事务参与者准备提交或者取消事务，然后进入表决过程。在表决过程中，参与者告知协调者自己的决策：同意（事务本地执行成功）或者取消（事务本地执行失败）。</p>\n<p>阶段2：提交阶段。在提交阶段，协调者基于第一阶段的投票结果进行决策：提交或者取消。当且仅当所有的参与者同意提交事务协调者才通知所有的参与者提交事务，否则通知所有参与者取消事务。参与者在接收到协调者的消息后执行相应操作。</p>\n<p>二阶段提交协议看起来确实能够提供原子性的操作，但还是存在几个缺点：</p>\n<ol>\n<li><p>同步阻塞问题。执行过程中，所有参与者都是事务阻塞型的。当参与者占有公共资源时，其他第三方节点访问公共资源不得不处于阻塞状态。</p>\n</li>\n<li><p>单点故障。由于协调者的重要性，一旦协调者发生故障，参与者会一直阻塞下去。尤其在第二阶段，协调者发生故障，那么所有的参与者还都处于锁定事务资源的状态中而无法继续完成事务操作。（如果是协调者挂掉，可以重新选举一个协调者，但是无法解决因为协调者宕机导致的参与者处于阻塞状态的问题。）</p>\n</li>\n<li><p>数据不一致。在二阶段提交的阶段二中，当协调者向参与者发送commit请求之后，发生了局部网络异常或者在发送commit请求过程中协调者发生了故障，导致只有一部分参与者接受到了commit请求。而在这部分参与者接到commit请求之后就会执行commit操作。但是其他部分未接到commit请求的机器则无法执行事务提交。于是整个分布式系统便出现了数据不一致性的现象。</p>\n</li>\n<li><p>二阶段无法解决的问题：协调者再发出commit消息之后宕机，而唯一接收到这条消息的参与者同时也宕机了。那么即使协调者通过选举协议产生了新的协调者，这条事务的状态也是不确定的，没人知道事务是否被已经提交。</p>\n</li>\n</ol>\n<p>上述协议存在问题，在请求阶段，如果某一参与者迟迟不能回复，系统将一直处于等待状态，并且其他参与者持有的资源不能得到释放。不过协调者可以通过引入事务的超时机制防止资源一直不能得到释放的情况。但是若协调者出现故障，事务将被阻塞。故两阶段提交协议可能面临两种故障：</p>\n<ol>\n<li><p>事务参与者发生故障。给每个事务设置超时时间，若参与者不能及时响应，整个事务失败。</p>\n</li>\n<li><p>协调者发生故障。协调者需要将事务相关信息记录到操作日志并同步到备用协调者，假如协调者发生故障，备用协调者可以接替完成后续工作。</p>\n</li>\n</ol>\n<p>总之，两阶段提交协议是阻塞协议，协议过程中需要锁住其它更新，且不能容错，并不适合高并发系统，大多数分布式系统都不采用，放弃对分布式事务的支持。</p>\n<h2 id=\"2-三阶段提交协议\"><a href=\"#2-三阶段提交协议\" class=\"headerlink\" title=\"2. 三阶段提交协议\"></a>2. 三阶段提交协议</h2><p>三阶段提交协议（Three-phase Commit，3PC）是对二阶段提交协议的改进，改动点如下：</p>\n<ol>\n<li><p>引入超时机制。同时在协调者和参与者中都引入超时机制。</p>\n</li>\n<li><p>在第一阶段和第二阶段中插入一个准备阶段。保证了在最后提交阶段之前各参与节点的状态是一致的。即3PC把2PC的准备阶段一分为二，这样三阶段提交就有CanCommit、PreCommit、DoCommit三个阶段。</p>\n</li>\n</ol>\n<p>在DoCommit阶段，如果参与者无法及时接收到来自协调者的doCommit或者rebort请求时，会在等待超时之后，会继续进行事务的提交。（其实这个应该是基于概率来决定的，当进入第三阶段时，说明参与者在第二阶段已经收到了PreCommit请求，那么协调者产生PreCommit请求的前提条件是他在第二阶段开始之前，收到所有参与者的CanCommit响应都是Yes。（一旦参与者收到了PreCommit，意味他知道大家其实都同意修改了）所以，一句话概括就是，当进入第三阶段时，由于网络超时等原因，虽然参与者没有收到commit或者abort响应，但是他有理由相信：成功提交的几率很大。 ）</p>\n<p>相对于2PC，3PC主要解决的单点故障问题，并减少阻塞，因为一旦参与者无法及时收到来自协调者的信息之后，他会默认执行commit。而不会一直持有事务资源并处于阻塞状态。但是这种机制也会导致数据一致性问题，因为，由于网络原因，协调者发送的abort响应没有及时被参与者接收到，那么参与者在等待超时之后执行了commit操作。这样就和其他接到abort命令并执行回滚的参与者之间存在数据不一致的情况。了解了2PC和3PC之后，我们可以发现，无论是二阶段提交还是三阶段提交都无法彻底解决分布式的一致性问题。</p>\n<p><a href=\"http://blog.jobbole.com/89140/\" target=\"_blank\" rel=\"noopener\">使用消息队列来避免分布式事务</a></p>\n<h2 id=\"3-Paxos协议\"><a href=\"#3-Paxos协议\" class=\"headerlink\" title=\"3.Paxos协议\"></a>3.Paxos协议</h2><p>Paxos协议用于解决多个节点之间的一致性问题。多个节点通过操作日志同步数据，如果只有一个节点为主节点，很容易确保多个节点之间操作日志的一致性。考虑主节点可能发生故障，系统需要选举新的主节点。Paxos协议正是用来实现此需求。只要保证多个节点之间操作日志的一致性，就能在这些节点上构建高可用的全局服务，例如分布式锁服务，全局命名和配置服务等。</p>\n<p>为了实现高可用性，主节点把数据以操作日志的形式同步到备节点。若主节点发生故障，备节点会提议自己成为主节点。存在的一个问题是网络分区的时候，可能会存在多个备节点（Proposer,提议者）提议。Paxos协议保证，即使存在多个提议者，也能保证所有节点最终达成一致，即选举出唯一的主节点。</p>\n<p>大多数情况下，系统只有一个提议节点，提议会很快被大多数节点接受。Paxos协议执行步骤如下：</p>\n<ol>\n<li><p>批准（accept）：提议者发送accept消息要求其他所有节点接受某个提议值，其它节点可以接受或者拒绝。</p>\n</li>\n<li><p>确认（acknowledge）：如果超过一半的其它节点接受，意味着提议值生效，提议者发送确认消息通知所有的节点提议生效。</p>\n</li>\n</ol>\n<p>当网络或者其他异常时，系统中可能存在多个提议者，各自发起不同的提议。这里的提议可以是一个修改，也可以是自己成为主节点。如果提议者第一次发起的accept请求没有被其它节点的多数派批准，那么需要完整地执行一轮Paxos协议。过程如下:</p>\n<ol>\n<li><p>准备（prepare）：Proposer首先选择一个提议序号n给其他的acceptor节点发送prepare消息。Acceptor收到prepare消息后，如果提议的序号大于其已经回复的所有prepare消息，则acceptor将自己上次接受的提议回复给proposer，并承诺不再回复小于n的提议。</p>\n</li>\n<li><p>批准（accept）：Proposer收到了acceptor中的多数派对prepare的回复后，就进入批准阶段。如果在之前的prepare阶段acceptor回复了上次接受的提议，那么proposer选择其中序号最大的提议值发给acceptor批准；否则，proposer生成一个新的提议值发给acceptor批准。Acceptor在不违背他之前在prepare阶段的承诺的前提下，接受这个请求。</p>\n</li>\n<li><p>确认（acknowledge）：如果超过一半的acceptor接受，提议值生效。Proposer发送knowledge消息通知所有的acceptor提议生效。</p>\n</li>\n</ol>\n<p>Paxos协议需要考虑两个问题：正确性，即只有一个提议值生效；可终止性，即最后总有一个提议值生效。</p>\n<p><strong>参考</strong>：</p>\n<ol>\n<li>杨传辉，《大规模分布式存储系统：原理解析与架构实战》</li>\n<li><a href=\"http://www.hollischuang.com/archives/681\" target=\"_blank\" rel=\"noopener\">关于分布式事务、两阶段提交协议、三阶提交协议</a></li>\n<li><a href=\"https://zh.wikipedia.org/wiki/Paxos%E7%AE%97%E6%B3%95\" target=\"_blank\" rel=\"noopener\">Paxos算法</a></li>\n<li><a href=\"http://www.hollischuang.com/archives/693\" target=\"_blank\" rel=\"noopener\">分布式一致性算法——paxos</a></li>\n</ol>\n","categories":["技术"],"tags":["分布式存储"]},{"title":"数据库主键、外键、索引","url":"http://yoursite.com/2018/07/26/数据库主键、外键、索引/","content":"<h2 id=\"1-主键\"><a href=\"#1-主键\" class=\"headerlink\" title=\"1. 主键\"></a>1. 主键</h2><p>表通常具有包含唯一标识表中每一行的值的一列或一组列。这样的一列或多列称为表的主键(PK)，用于强制表的实体完整性。由于主键约束可保证数据的唯一性，因此经常对标识列定义这种约束。</p>\n<p>如果为表指定了主键约束，数据库引擎将通过为主键列自动创建唯一索引来强制数据的唯一性。当在查询中使用主键时，此索引还允许对数据进行快速访问。如果对多列定义了主键约束，则一列中的值可能会重复，但来自主键约束定义中所有列的值的任何组合必须唯一。</p>\n<h2 id=\"2-外键\"><a href=\"#2-外键\" class=\"headerlink\" title=\"2. 外键\"></a>2. 外键</h2><p>外键(FK)是用于在两个表中的数据之间建立和加强链接的一列或多列的组合，可控制可在外键表中存储的数据。在外键引用中，当包含一个表的主键值的一个或多个列被另一个表中的一个或多个列引用时，就在这两个表之间创建了链接。这个列就成为第二个表的外键。</p>\n<blockquote>\n<p>互联网行业应用不推荐使用外键： 用户量大，并发度高，为此数据库服务器很容易成为性能瓶颈，尤其受IO能力限制，且不能轻易地水平扩展。</p>\n</blockquote>\n<h2 id=\"3-索引\"><a href=\"#3-索引\" class=\"headerlink\" title=\"3. 索引\"></a>3. 索引</h2><p>一个索引是存储的表中一个特定列的值数据结构（最常见的是B-Tree）。索引是在表的列上创建。所以，要记住的关键点是索引包含一个表中列的值，并且这些值存储在一个数据结构中，索引存储了指向表中某一行的指针。请记住记住这一点：<strong>索引是一种数据结构</strong>。</p>\n<ul>\n<li><p><strong>B-Tree</strong>是最常用的用于索引的数据结构。因为它们是时间复杂度低，查找、删除、插入操作都可以可以在对数时间内完成。另外一个重要原因存储在B-Tree中的数据是有序的。数据库管理系统（RDBMS）通常决定索引应该用哪些数据结构。但是，在某些情况下，你在创建索引时可以指定索引要使用的数据结构。</p>\n</li>\n<li><p><strong>哈希表</strong>是另外一种你可能看到用作索引的数据结构-这些索引通常被称为哈希索引。使用哈希索引的原因是，在寻找值时哈希表效率极高。但只适合查询键值对-也就是说查询相等的查询（例：like “WHERE name = ‘Jesus’）。哈希表的键值映射也暗示其键的存储是无序的，这就是为什么哈希索引通常不是数据库索引的默认数据结构-因为在作为索引的数据结构时，其不像B-Tree那么灵活。</p>\n</li>\n</ul>\n<h2 id=\"4-主键、外键和索引的对比\"><a href=\"#4-主键、外键和索引的对比\" class=\"headerlink\" title=\"4. 主键、外键和索引的对比\"></a>4. 主键、外键和索引的对比</h2><table>\n<thead>\n<tr>\n<th></th>\n<th>主键</th>\n<th>外键</th>\n<th>索引   </th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td><strong>定义</strong></td>\n<td>唯一标识一条记录，不允许重复，不允许为空</td>\n<td>表的外键是另一表的主键,外键可以重复, 可以为空</td>\n<td>该字段没有重复值，但可以有一个空值</td>\n</tr>\n<tr>\n<td><strong>作用</strong></td>\n<td>保证数据完整性</td>\n<td>和其他表建立联系</td>\n<td>提高查询排序速度 </td>\n</tr>\n<tr>\n<td><strong>个数</strong></td>\n<td>唯一</td>\n<td>可以有多个外键</td>\n<td>可以有多个唯一索引 </td>\n</tr>\n</tbody>\n</table>\n<p><strong>参考</strong>：</p>\n<ol>\n<li><a href=\"https://blog.csdn.net/weiliangliang111/article/details/51333169\" target=\"_blank\" rel=\"noopener\">数据库索引到底是什么，是怎样工作的？</a></li>\n</ol>\n","categories":["技术"],"tags":["数据库"]},{"title":"世界杯一二事","url":"http://yoursite.com/2018/07/16/世界杯一二事/","content":"<h2 id=\"1-比赛前的选择\"><a href=\"#1-比赛前的选择\" class=\"headerlink\" title=\"1. 比赛前的选择\"></a>1. 比赛前的选择</h2><p>昨天熬夜看球到2点，见证了法国夺冠。兴奋，满足，那种感觉就是你一直坚信一件事情的结果，而结局如你所愿。<br>世界杯开始前，微博有一个活动，pick你认为的今年的冠军。对一个伪足球迷而言，很难讲自己最喜欢哪只球队，德国，巴西，法国，阿根廷，西班牙，实力强的队伍太多了，各有各的特点，从某种意义上讲都有夺冠的机会。但是冠军只有一个，你的选择也只有一个。最终选择了法国，没有太多的理由，大概是年轻有潜力吧。既然选择了，就仿佛是认定了一般，开始了解法国队，开始关注法国的每一场比赛，关注某些球星的表现。最终，法国也确实没有让球迷失望，时隔20年再次捧起大力神杯。</p>\n<p><img src=\"http://ojr4jzv5e.bkt.clouddn.com/worldcup_1.jpg\"></p>\n<p>选择需要理性，但有时候也存在着感性。法国队球员身价最高，年轻，然而缺少经验，这是一直不被大家看好的一点。但没有一支球队是完美的，何况完美的球队也不一定能夺得最终的冠军，竞技比赛充满太多未知性。但是在选择的那一刻，你认准一点或许就足够了，足够你做出一个选择。</p>\n<h2 id=\"2-直到有一天，世界杯成为记忆\"><a href=\"#2-直到有一天，世界杯成为记忆\" class=\"headerlink\" title=\"2. 直到有一天，世界杯成为记忆\"></a>2. 直到有一天，世界杯成为记忆</h2><p>翻看微信公众号，突然就看到了杨毅先生的文章《<a href=\"https://mp.weixin.qq.com/s/2oKKP4hVaFmGJsQpmXDNrg\" target=\"_blank\" rel=\"noopener\">直到有一天，世界杯成为记忆</a>》，感触颇深。俄罗斯世界杯是我看的第三届世界杯，严格意义上我并不是足球迷，但是世界杯这样的体坛盛事，对每个热爱体育竞技的人都不容错过。很喜欢文中的几段话，“不夸张地说，对一个喜爱体育的男生来说，在一个更长的跨度里，没有什么能超过世界杯带来的仪式感。这是体育世界里最大的，最富有世界影响力的品牌，四年才一度。既体现它的稀有，又映衬着你生活里不同的时代。你会轻易地记起，哪一届世界杯你有多大，你正在经历什么。每一届时的你都截然不同，因为每一个四年都是你成长的刻度。”</p>\n<p><img src=\"http://ojr4jzv5e.bkt.clouddn.com/worldcup_02.jpg\"></p>\n<p>的确，每一届世界杯都是一个阶段，世界杯结束一个阶段结束了。10年第一次看世界杯，那会儿刚初中毕业，印象最深的是德国阿根廷那场；14年在大学，凌晨4点和室友看德国巴西，以及自己一个人在车站附近网吧看了德国阿根廷；18年读研，在家看法国阿根廷，在宾馆看法国比利时，在寝室看法国克罗地亚。4年时间说长不长，但也不短，不知道还能看几届世界杯，只希望今后日子里心怀热情，但行好事，莫问前程。</p>\n","categories":["生活"],"tags":["生活"]},{"title":"《统计学习方法》总结","url":"http://yoursite.com/2018/07/13/《统计学习方法》总结/","content":"<p>[TOC]</p>\n<p>本书主要介绍了感知机、K近邻法、朴素贝叶斯法、决策树、Logistic回归与最大熵模型、支持向量机、提升方法、EM算法、隐马尔可夫模型、条件随机场。</p>\n<h3 id=\"1-各种统计学习方法特点总结\"><a href=\"#1-各种统计学习方法特点总结\" class=\"headerlink\" title=\"1. 各种统计学习方法特点总结\"></a>1. 各种统计学习方法特点总结</h3><center><strong>表1 10种统计学习方法特点的概括总结</strong></center>\n\n<p><br></p>\n<table>\n<thead>\n<tr>\n<th>方法</th>\n<th>适用问题</th>\n<th>模型特点</th>\n<th>模型类型</th>\n<th>学习策略</th>\n<th>学习的损失函数</th>\n<th>学习算法 </th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>感知机</td>\n<td>二类分类</td>\n<td>分离超平面</td>\n<td>判别模型</td>\n<td>极小化误分点到超平面的距离</td>\n<td>误分点到超平面的距离</td>\n<td>随机梯度下降</td>\n</tr>\n<tr>\n<td>k临近法</td>\n<td>多类分类，回归</td>\n<td>特征空间，样本点</td>\n<td>判别模型</td>\n<td></td>\n<td></td>\n<td></td>\n</tr>\n<tr>\n<td>朴素贝叶斯法</td>\n<td>多类分类</td>\n<td>特征与类别的联合概率分布，条件独立假设</td>\n<td>生成模型</td>\n<td>极大似然估计，极大后验概率估计</td>\n<td>对数似然损失</td>\n<td>概率计算公式，EM算法</td>\n</tr>\n<tr>\n<td>决策树</td>\n<td>多类分类，回归</td>\n<td>分类树，回归树</td>\n<td>判别模型</td>\n<td>正则化的极大似然估计</td>\n<td>对数似然损失</td>\n<td>特征选择，生成，剪枝</td>\n</tr>\n<tr>\n<td>Logistic回归与最大熵模型</td>\n<td>多类分类</td>\n<td>特征条件下类别的条件概率分布，对数线性模型</td>\n<td>判别模型</td>\n<td>极大似然估计，正则化的极大似然估计</td>\n<td>logistic损失</td>\n<td>改进的迭代尺度算法，梯度下降，拟牛顿法</td>\n</tr>\n<tr>\n<td>支持向量机</td>\n<td>二类分类</td>\n<td>分离超平面，核技巧</td>\n<td>判别模型</td>\n<td>极小化正则化合页损失，软间隔最大化</td>\n<td>合页损失</td>\n<td>序列最小最优化算法（SMO）</td>\n</tr>\n<tr>\n<td>提升方法</td>\n<td>二类分类</td>\n<td>弱分类器的线性组合</td>\n<td>判别模型</td>\n<td>极小化加法模型的指数损失</td>\n<td>指数损失</td>\n<td>前向分步加法算法</td>\n</tr>\n<tr>\n<td>EM算法</td>\n<td>概率模型参数估计</td>\n<td>含隐变量概率模型</td>\n<td></td>\n<td>极大似然估计，极大后验概率估计</td>\n<td>对数似然损失</td>\n<td>迭代算法</td>\n</tr>\n<tr>\n<td>隐马尔可夫模型</td>\n<td>标注</td>\n<td>观测序列与状态序列的联合概率分布模型</td>\n<td>生成模型</td>\n<td>极大似然估计，极大后验概率估计</td>\n<td>对数似然损失</td>\n<td>概率计算公式，EM算法</td>\n</tr>\n<tr>\n<td>条件随机场</td>\n<td>标注</td>\n<td>状态序列条件下观测序列的条件概率分布，对数线性模型</td>\n<td>判别模型</td>\n<td>极大似然估计，正则化极大似然估计</td>\n<td>对数似然损失</td>\n<td>改进的迭代尺度算法，梯度下降，拟牛顿法</td>\n</tr>\n</tbody>\n</table>\n<p><strong>Relevant Link</strong></p>\n<ol>\n<li><a href=\"https://zhuanlan.zhihu.com/p/25327755\" target=\"_blank\" rel=\"noopener\">机器学习算法集锦：从贝叶斯到深度学习及各自优缺点</a></li>\n<li><a href=\"https://www.zhihu.com/question/26726794\" target=\"_blank\" rel=\"noopener\">各种机器学习算法的应用场景分别是什么（比如朴素贝叶斯、决策树、K 近邻、SVM、逻辑回归最大熵模型）？</a></li>\n</ol>\n<h3 id=\"2-感知机\"><a href=\"#2-感知机\" class=\"headerlink\" title=\"2. 感知机\"></a>2. 感知机</h3><p>感知机（perceptron）是<strong>二类分类</strong>的<strong>线性分类</strong>模型，其输入为样本实例的特征向量，输出为实例的类别，是一种<strong>判别模型</strong>。感知机的目的是求出将训练数据进行线性划分的超平面，为此，导入基于误分类的损失函数，利用梯度下降法对损失函数进行最小化，以求得感知机模型。感知机学习算法具有简单易实现的优点，分为<strong>原始形式</strong>和<strong>对偶形式</strong>。</p>\n<h4 id=\"2-1-感知机模型与学习策略\"><a href=\"#2-1-感知机模型与学习策略\" class=\"headerlink\" title=\"2.1 感知机模型与学习策略\"></a>2.1 感知机模型与学习策略</h4><h6 id=\"2-1-1-数据集的线性可分性\"><a href=\"#2-1-1-数据集的线性可分性\" class=\"headerlink\" title=\"2.1.1 数据集的线性可分性\"></a>2.1.1 数据集的线性可分性</h6><p>存在一个超平面将数据集的正实例点和负实例点完全正确地划分到超平面的两侧，则称该数据集线性可分。$2*w=12$</p>\n<h6 id=\"2-1-2-感知机学习策略\"><a href=\"#2-1-2-感知机学习策略\" class=\"headerlink\" title=\"2.1.2 感知机学习策略\"></a>2.1.2 感知机学习策略</h6><p>定义（经验）损失函数并将损失函数最小化，损失函数的一个自然选择是误分类的点数，但是这样的损失函数不是参数w，b的连续可导函数，不易优化。因此选择误分类点到超平面S的总距离。<br>定义感知机 $ [sign(w*x+b)] $ 学习的损失函数为</p>\n<p><img src=\"https://latex.codecogs.com/gif.latex?\\small&space;L(w,b)=-\\sum_{x_{i}\\in&space;M}y_{i}(w*x_{i}&plus;b)\" title=\"\\small L(w,b)=-\\sum_{x_{i}\\in M}y_{i}(w*x_{i}+b)\"></p>\n<h4 id=\"2-2-感知机学习算法\"><a href=\"#2-2-感知机学习算法\" class=\"headerlink\" title=\"2.2 感知机学习算法\"></a>2.2 感知机学习算法</h4><p>感知机学习问题转化为求解损失函数的最优化问题，最优化问题的方法是随机梯度下降法。首先，任意选取一个超平面，然后采用梯度下降法不断地极小化目标函数，极小化过程不是一次使所有误分类点的梯度下降，而是一次随机选取一个误分类点使其梯度下降。</p>\n<h6 id=\"2-2-1-感知机学习算法的原始形式\"><a href=\"#2-2-1-感知机学习算法的原始形式\" class=\"headerlink\" title=\"2.2.1 感知机学习算法的原始形式\"></a>2.2.1 感知机学习算法的原始形式</h6><h6 id=\"2-2-2-算法的收敛性\"><a href=\"#2-2-2-算法的收敛性\" class=\"headerlink\" title=\"2.2.2 算法的收敛性\"></a>2.2.2 算法的收敛性</h6><p>Novikoff定理表明误分类的次数是有上界的，经过有限次搜索可以找到将训练数据完全分开的分离超平面。<br>感知机学习算法存在许多解，这些解既依赖于初值的选择，也依赖于迭代过程中误分类点的选择顺序。</p>\n<h6 id=\"2-2-3-感知机学习算法的对偶形式\"><a href=\"#2-2-3-感知机学习算法的对偶形式\" class=\"headerlink\" title=\"2.2.3 感知机学习算法的对偶形式\"></a>2.2.3 感知机学习算法的对偶形式</h6><h3 id=\"3-K近邻法\"><a href=\"#3-K近邻法\" class=\"headerlink\" title=\"3. K近邻法\"></a>3. K近邻法</h3><p>K近邻法是一种基本分类与回归方法，K近邻法输入为实例的特征向量，对应于特征空间的点；输出为实例的类别，可以取多类。分类时，对新的实例，根据其k个最邻近的训练实例的类别，通过多数表决方式进行预测。K近邻法不具有显式的学习过程，本质上是利用训练数据对特征向量空间进行划分，并作为其分类的“模型”。K值的选择、距离度量以及分类决策规则是K近邻法的三个基本要素。</p>\n<h4 id=\"1-K近邻模型\"><a href=\"#1-K近邻模型\" class=\"headerlink\" title=\".1 K近邻模型\"></a>.1 K近邻模型</h4><h6 id=\"3-1-1-距离度量\"><a href=\"#3-1-1-距离度量\" class=\"headerlink\" title=\"3.1.1 距离度量\"></a>3.1.1 距离度量</h6><p>采用不同的距离度量，如欧氏距离、Minkowski距离等，所确定的最近邻点也是不同的。</p>\n<h6 id=\"3-1-2-K值选择\"><a href=\"#3-1-2-K值选择\" class=\"headerlink\" title=\"3.1.2 K值选择\"></a>3.1.2 K值选择</h6><p>K值的选择影响近似误差和估计误差。K值一般选择一个比较小的数值，通常采用交叉验证法选取最优的K值。</p>\n<h6 id=\"3-1-3-分类决策规则\"><a href=\"#3-1-3-分类决策规则\" class=\"headerlink\" title=\"3.1.3 分类决策规则\"></a>3.1.3 分类决策规则</h6><p>多数表决。对数表决规则等价于经验风险最小化。</p>\n<h4 id=\"3-2-K近邻法的实现：kd树\"><a href=\"#3-2-K近邻法的实现：kd树\" class=\"headerlink\" title=\"3.2 K近邻法的实现：kd树\"></a>3.2 K近邻法的实现：kd树</h4><h3 id=\"4-朴素贝叶斯\"><a href=\"#4-朴素贝叶斯\" class=\"headerlink\" title=\"4. 朴素贝叶斯\"></a>4. 朴素贝叶斯</h3><p>朴素贝叶斯法是基于<strong>贝叶斯原理</strong>与<strong>特征条件独立假设</strong>的分类方法。对于给定的训练数据集，首先基于特征条件独立假设学习输入/输出的<strong>联合概率分布</strong>；然后基于此模型，对给定的输入x,利用贝叶斯原理求出后验概率最大的输出y。</p>\n<h4 id=\"4-1-朴素贝叶斯法的学习与分类\"><a href=\"#4-1-朴素贝叶斯法的学习与分类\" class=\"headerlink\" title=\"4.1 朴素贝叶斯法的学习与分类\"></a>4.1 朴素贝叶斯法的学习与分类</h4><h4 id=\"4-2-朴素贝叶斯法的参数估计\"><a href=\"#4-2-朴素贝叶斯法的参数估计\" class=\"headerlink\" title=\"4.2 朴素贝叶斯法的参数估计\"></a>4.2 朴素贝叶斯法的参数估计</h4><h6 id=\"4-2-1-极大似然估计\"><a href=\"#4-2-1-极大似然估计\" class=\"headerlink\" title=\"4.2.1 极大似然估计\"></a>4.2.1 极大似然估计</h6><h6 id=\"4-2-2-贝叶斯估计\"><a href=\"#4-2-2-贝叶斯估计\" class=\"headerlink\" title=\"4.2.2 贝叶斯估计\"></a>4.2.2 贝叶斯估计</h6><p>用极大似然估计可能会出现所要估计的概率值为0的情况，会影响后验概率的计算结果，使分类产生偏差。条件概率的贝叶斯估计如下：<br><img src=\"https://latex.codecogs.com/gif.latex?P_{\\lambda&space;}(X^{(j)}=a_{jl}|Y=c_{k})=\\frac{\\sum_{i=1}^{N}I(x_{i}^{(j)}=a_{jl},y_{i}=c_{k})&plus;\\lambda}{\\sum_{i=1}^{N}I(y_{i}=c_{k})&plus;S_{j}\\lambda&space;}\" title=\"P_{\\lambda }(X^{(j)}=a_{jl}|Y=c_{k})=\\frac{\\sum_{i=1}^{N}I(x_{i}^{(j)}=a_{jl},y_{i}=c_{k})+\\lambda}{\\sum_{i=1}^{N}I(y_{i}=c_{k})+S_{j}\\lambda }\"><br>式中$[\\lambda]$&gt;=0.等价于在随机变量各个取值的频数上赋予一个正数$[\\lambda]$&gt;=0。当$[\\lambda]$=0时就是极大似然估计。常取$[\\lambda]$=1，此时称为拉普拉斯平滑。</p>\n<h3 id=\"5-决策树\"><a href=\"#5-决策树\" class=\"headerlink\" title=\"5. 决策树\"></a>5. 决策树</h3><h4 id=\"5-1-决策树模型与学习\"><a href=\"#5-1-决策树模型与学习\" class=\"headerlink\" title=\"5.1 决策树模型与学习\"></a>5.1 决策树模型与学习</h4><h4 id=\"5-2-特征选择\"><a href=\"#5-2-特征选择\" class=\"headerlink\" title=\"5.2 特征选择\"></a>5.2 特征选择</h4><h4 id=\"5-3-决策树的生成\"><a href=\"#5-3-决策树的生成\" class=\"headerlink\" title=\"5.3 决策树的生成\"></a>5.3 决策树的生成</h4><h4 id=\"5-4-决策树的剪枝\"><a href=\"#5-4-决策树的剪枝\" class=\"headerlink\" title=\"5.4 决策树的剪枝\"></a>5.4 决策树的剪枝</h4><h4 id=\"5-5-CART算法\"><a href=\"#5-5-CART算法\" class=\"headerlink\" title=\"5.5 CART算法\"></a>5.5 CART算法</h4><h3 id=\"6-Logistic回归与最大熵模型\"><a href=\"#6-Logistic回归与最大熵模型\" class=\"headerlink\" title=\"6. Logistic回归与最大熵模型\"></a>6. Logistic回归与最大熵模型</h3><h4 id=\"6-1-Logistic回归模型\"><a href=\"#6-1-Logistic回归模型\" class=\"headerlink\" title=\"6.1 Logistic回归模型\"></a>6.1 Logistic回归模型</h4><h4 id=\"6-2-最大熵模型\"><a href=\"#6-2-最大熵模型\" class=\"headerlink\" title=\"6.2 最大熵模型\"></a>6.2 最大熵模型</h4><h4 id=\"6-3-模型学习的最优化算法\"><a href=\"#6-3-模型学习的最优化算法\" class=\"headerlink\" title=\"6.3 模型学习的最优化算法\"></a>6.3 模型学习的最优化算法</h4><h3 id=\"7-支持向量机\"><a href=\"#7-支持向量机\" class=\"headerlink\" title=\"7. 支持向量机\"></a>7. 支持向量机</h3><h4 id=\"7-1-线性可分向量机与硬间隔最大化\"><a href=\"#7-1-线性可分向量机与硬间隔最大化\" class=\"headerlink\" title=\"7.1 线性可分向量机与硬间隔最大化\"></a>7.1 线性可分向量机与硬间隔最大化</h4><h4 id=\"7-2-线性支持向量机与软间隔最大化\"><a href=\"#7-2-线性支持向量机与软间隔最大化\" class=\"headerlink\" title=\"7.2 线性支持向量机与软间隔最大化\"></a>7.2 线性支持向量机与软间隔最大化</h4><h4 id=\"7-3-非线性支持向量机与核函数\"><a href=\"#7-3-非线性支持向量机与核函数\" class=\"headerlink\" title=\"7.3 非线性支持向量机与核函数\"></a>7.3 非线性支持向量机与核函数</h4><h4 id=\"7-4-序列最小最优化算法\"><a href=\"#7-4-序列最小最优化算法\" class=\"headerlink\" title=\"7.4 序列最小最优化算法\"></a>7.4 序列最小最优化算法</h4><h3 id=\"8-提升方法\"><a href=\"#8-提升方法\" class=\"headerlink\" title=\"8. 提升方法\"></a>8. 提升方法</h3><h3 id=\"9-EM算法\"><a href=\"#9-EM算法\" class=\"headerlink\" title=\"9. EM算法\"></a>9. EM算法</h3><h3 id=\"10-隐马尔科夫模型\"><a href=\"#10-隐马尔科夫模型\" class=\"headerlink\" title=\"10. 隐马尔科夫模型\"></a>10. 隐马尔科夫模型</h3><h3 id=\"11-条件随机场\"><a href=\"#11-条件随机场\" class=\"headerlink\" title=\"11. 条件随机场\"></a>11. 条件随机场</h3><p><strong>值得思考的问题</strong></p>\n<ol>\n<li><a href=\"https://www.zhihu.com/question/26726794\" target=\"_blank\" rel=\"noopener\">各种机器学习算法的应用场景分别是什么（比如朴素贝叶斯、决策树、K 近邻、SVM、逻辑回归最大熵模型）？</a></li>\n<li><a href=\"https://www.zhihu.com/question/51500780\" target=\"_blank\" rel=\"noopener\">感知机（perceptron）和支持向量机（svm）是一种东西吗？如果不是那他们的区别和关系是什么？</a></li>\n<li><a href=\"https://www.zhihu.com/question/49805962\" target=\"_blank\" rel=\"noopener\">如何用简单易懂的例子解释格拉姆矩阵/Gram matrix？</a></li>\n<li><a href=\"https://www.zhihu.com/question/60793482\" target=\"_blank\" rel=\"noopener\">如何理解和区分近似误差和估计误差?</a></li>\n<li><a href=\"https://www.zhihu.com/question/20507061\" target=\"_blank\" rel=\"noopener\">线性代数中，特征值与特征向量在代数和几何层面的实际意义是什么？</a></li>\n<li><a href=\"https://www.zhihu.com/question/20447622\" target=\"_blank\" rel=\"noopener\">最大似然估计和最小二乘法怎么理解？</a></li>\n<li><a href=\"https://www.zhihu.com/question/20852004\" target=\"_blank\" rel=\"noopener\">如何通俗易懂地解释「协方差」与「相关系数」的概念？</a></li>\n<li><a href=\"https://www.zhihu.com/question/263672028\" target=\"_blank\" rel=\"noopener\">为什么都说神经网络是个黑箱？</a></li>\n</ol>\n","categories":["技术"],"tags":["机器学习"]},{"title":"about","url":"http://yoursite.com/about/index.html","content":"<p>about:</p>\n<ul>\n<li>type: me<br>icon: icon-user<br>text_value:<ul>\n<li>“”<!-- - \"喜欢NBA.\" -->\n<!-- - \"前端开发工程师，常用 HTML / CSS / JavaScript.\" --></li>\n</ul>\n</li>\n<li>type: Github<br>icon: icon-github<br>text_key: Github<br>text_value: “@zer0ing”<br>text_value_url: <a href=\"https://github.com/zer0ing\" target=\"_blank\" rel=\"noopener\">https://github.com/zer0ing</a></li>\n<li>type: weibo<br>icon: icon-weibo<br>text_key: 微博<br>text_value: “@_zeroyoung”<br>text_value_url: <a href=\"https://weibo.com/u/5452588095\" target=\"_blank\" rel=\"noopener\">https://weibo.com/u/5452588095</a></li>\n<li>type: mail<br>icon: icon-mail<br>text_key: Gmail<br>text_value: “<a href=\"mailto:zeroing.young@gmail.com\" target=\"_blank\" rel=\"noopener\">zeroing.young@gmail.com</a>“</li>\n<li>type: location<br>icon: icon-location<br>text_value: 成都</li>\n</ul>\n","categories":[],"tags":[]},{"title":"categories","url":"http://yoursite.com/categories/index.html","content":"","categories":[],"tags":[]},{"title":"category","url":"http://yoursite.com/category/index.html","content":"","categories":[],"tags":[]},{"title":"link","url":"http://yoursite.com/link/index.html","content":"<p>link:</p>\n<ul>\n<li>name: 织网<br>info: 身体和灵魂，总有一个在路上<br>url: <a href=\"http://zheng-ji.info/\" target=\"_blank\" rel=\"noopener\">http://zheng-ji.info/</a><br>avatar: <a href=\"https://avatars3.githubusercontent.com/u/1414745?v=3&amp;s=460\" target=\"_blank\" rel=\"noopener\">https://avatars3.githubusercontent.com/u/1414745?v=3&amp;s=460</a></li>\n<li>name: Dongyado<br>info: 生命不止，折腾不息<br>url: <a href=\"http://dongyado.com/\" target=\"_blank\" rel=\"noopener\">http://dongyado.com/</a><br>avatar: <a href=\"https://avatars0.githubusercontent.com/u/6274940?v=3&amp;s=460\" target=\"_blank\" rel=\"noopener\">https://avatars0.githubusercontent.com/u/6274940?v=3&amp;s=460</a></li>\n<li>name: OrangeCoder<br>info: android ffmpeg nodejs gradle<br>url: <a href=\"http://orangecoder.com/\" target=\"_blank\" rel=\"noopener\">http://orangecoder.com/</a><br>avatar: <a href=\"https://avatars0.githubusercontent.com/u/2263785?v=3&amp;s=460\" target=\"_blank\" rel=\"noopener\">https://avatars0.githubusercontent.com/u/2263785?v=3&amp;s=460</a></li>\n<li>name: EverET<br>info: 好记性不如烂笔头<br>url: <a href=\"http://everet.org/about-me/\" target=\"_blank\" rel=\"noopener\">http://everet.org/about-me/</a><br>avatar: <a href=\"https://avatars1.githubusercontent.com/u/1559563?v=3&amp;s=460\" target=\"_blank\" rel=\"noopener\">https://avatars1.githubusercontent.com/u/1559563?v=3&amp;s=460</a></li>\n</ul>\n","categories":[],"tags":[]},{"title":"schedule","url":"http://yoursite.com/schedule/index.html","content":"","categories":[],"tags":[]},{"title":"search","url":"http://yoursite.com/search/index.html","content":"","categories":[],"tags":[]},{"title":"tag","url":"http://yoursite.com/tag/index.html","content":"","categories":[],"tags":[]},{"title":"tags","url":"http://yoursite.com/tags/index.html","content":"","categories":[],"tags":[]},{"title":"sitemap","url":"http://yoursite.com/sitemap/index.html","content":"","categories":[],"tags":[]}]